import json
import numpy as np
import os
from typing import List, Dict, Any
from pathlib import Path

class JSONSearchService:
    def __init__(self, embedding_model, auto_load: bool = False, data_path: str = "./data/processed/"):
        self.embedding_model = embedding_model
        self.data_path = Path(data_path)
        self.documents = []
        
        # ğŸ‘ˆ í•µì‹¬ ë³€ê²½: auto_load=Falseë¡œ ìë™ ë¡œë”© ë¹„í™œì„±í™”
        if auto_load:
            self.load_all_documents()
    
    def load_all_documents(self):
        """ëª¨ë“  JSON ë¬¸ì„œ ë¡œë“œ (ê¸°ì¡´ ë°©ì‹ - ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)"""
        json_files = list(self.data_path.glob("*.json"))
        print(f"ğŸ“š JSON ë¬¸ì„œ ë¡œë“œ ì¤‘: {len(json_files)}ê°œ íŒŒì¼")
        
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                    if "sections" in json_data:
                        self.documents.append(json_data)
                        sections_count = len(json_data.get("sections", []))
                        print(f"  âœ… {json_file.name}: {sections_count}ê°œ ì„¹ì…˜ ë¡œë“œ")
            except Exception as e:
                print(f"  âŒ {json_file.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        total_sections = sum(len(doc.get("sections", [])) for doc in self.documents)
        print(f"ğŸ“– ì´ {total_sections}ê°œ ì„¹ì…˜ ë¡œë“œ ì™„ë£Œ")
    
    def add_document(self, json_data: Dict[str, Any]):
        """ìƒˆ JSON ë¬¸ì„œ ì¶”ê°€ (ì°¨ëŸ‰ë³„ ë‹¨ì¼ ë¬¸ì„œ)"""
        if "sections" in json_data:
            # ğŸ‘ˆ ì¤‘ìš”: ê¸°ì¡´ ë¬¸ì„œë¥¼ ëŒ€ì²´ (ì°¨ëŸ‰ë³„ë¡œ í•˜ë‚˜ì˜ ë¬¸ì„œë§Œ ìœ ì§€)
            self.documents = [json_data]
            sections_count = len(json_data.get("sections", []))
            vehicle_name = self._extract_vehicle_name_from_data(json_data)
            print(f"ğŸ“„ {vehicle_name} ë§¤ë‰´ì–¼ ì¶”ê°€: {sections_count}ê°œ ì„¹ì…˜")
    
    def _extract_vehicle_name_from_data(self, json_data: Dict[str, Any]) -> str:
        """JSON ë°ì´í„°ì—ì„œ ì°¨ëŸ‰ëª… ì¶”ì¶œ"""
        file_name = json_data.get("file_name", "")
        file_name_lower = file_name.lower()
        
        # ê·¸ëœì € í™•ì¸
        if "ê·¸ëœì €" in file_name or "granjer" in file_name_lower or "grandeur" in file_name_lower:
            if "hybrid" in file_name_lower or "í•˜ì´ë¸Œë¦¬ë“œ" in file_name:
                return "ê·¸ëœì € hybrid"
            return "ê·¸ëœì €"
        
        # ì‹¼íƒ€í˜ í™•ì¸
        elif "ì‹¼íƒ€í˜" in file_name or "santafe" in file_name_lower:
            return "ì‹¼íƒ€í˜"
        
        # ì˜ë‚˜íƒ€ í™•ì¸
        elif "ì˜ë‚˜íƒ€" in file_name or "sonata" in file_name_lower:
            if "hybrid" in file_name_lower or "í•˜ì´ë¸Œë¦¬ë“œ" in file_name:
                return "ì˜ë‚˜íƒ€ hybrid"
            return "ì˜ë‚˜íƒ€"
        
        # ì•„ë°˜ë–¼ í™•ì¸
        elif "ì•„ë°˜ë–¼" in file_name or "avante" in file_name_lower or "elantra" in file_name_lower:
            return "ì•„ë°˜ë–¼"
        
        # ì½”ë‚˜ í™•ì¸
        elif "ì½”ë‚˜" in file_name or "kona" in file_name_lower:
            if "electric" in file_name_lower or "ì¼ë ‰íŠ¸ë¦­" in file_name or "ev" in file_name_lower:
                return "ì½”ë‚˜ electric"
            return "ì½”ë‚˜"
        
        # íˆ¬ì‹¼ í™•ì¸
        elif "íˆ¬ì‹¼" in file_name or "tucson" in file_name_lower:
            if "hybrid" in file_name_lower or "í•˜ì´ë¸Œë¦¬ë“œ" in file_name:
                return "íˆ¬ì‹¼ hybrid"
            return "íˆ¬ì‹¼"
        
        # í ë¦¬ì„¸ì´ë“œ í™•ì¸ (íŒ°ë¦¬ì„¸ì´ë“œë„ í¬í•¨)
        elif "í ë¦¬ì„¸ì´ë“œ" in file_name or "íŒ°ë¦¬ì„¸ì´ë“œ" in file_name or "palisade" in file_name_lower:
            if "hybrid" in file_name_lower or "í•˜ì´ë¸Œë¦¬ë“œ" in file_name:
                return "í ë¦¬ì„¸ì´ë“œ hybrid"
            return "í ë¦¬ì„¸ì´ë“œ"
        
        return f"ì•Œ ìˆ˜ ì—†ëŠ” ì°¨ëŸ‰ ({file_name})"
    
    def search_sections(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """JSON ì„¹ì…˜ ê¸°ë°˜ ê²€ìƒ‰ (ë‹¨ì¼ ì°¨ëŸ‰ ë¬¸ì„œì—ì„œë§Œ)"""
        if not self.documents:
            print(f"âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        vehicle_name = self._extract_vehicle_name_from_data(self.documents[0])
        print(f"ğŸ” {vehicle_name} ë§¤ë‰´ì–¼ ê²€ìƒ‰ ì‹œì‘: '{query}'")
        
        search_results = []
        
        # ğŸ‘ˆ ë‹¨ì¼ ë¬¸ì„œ(ì„ íƒëœ ì°¨ëŸ‰)ì—ì„œë§Œ ê²€ìƒ‰
        doc = self.documents[0]
        filename = doc.get("file_name", "unknown.json")
        sections = doc.get("sections", [])
        
        for section in sections:
            scores = self._calculate_all_scores(query, section)
            total_score = self._calculate_total_score(scores)
            
            if total_score > 0.05:  # ì„ê³„ê°’
                search_results.append({
                    "score": total_score,
                    "source": filename,
                    "section_number": section.get("section_number", 0),
                    "title": section.get("title", ""),
                    "page_range": section.get("page_range", [0, 0]),
                    "content": section.get("content", ""),
                    "keywords": section.get("keywords", []),
                    "subsections": section.get("subsections", []),
                    "match_details": {
                        "title_score": round(scores["title"], 3),
                        "keyword_score": round(scores["keyword"], 3),
                        "content_score": round(scores["content"], 3),
                        "bonus_score": round(scores["bonus"], 3)
                    }
                })
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        search_results.sort(key=lambda x: x["score"], reverse=True)
        
        print(f"ğŸ“Š {vehicle_name} ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ì„¹ì…˜")
        for i, result in enumerate(search_results[:3]):
            print(f"  {i+1}. [{result['score']:.3f}] {result['title']} (í˜ì´ì§€ {result['page_range']})")
        
        return search_results[:k]
    
    def _calculate_all_scores(self, query: str, section: Dict) -> Dict[str, float]:
        """ëª¨ë“  ì ìˆ˜ ê³„ì‚°"""
        return {
            "title": self._calculate_title_score(query, section.get("title", "")),
            "keyword": self._calculate_keyword_score(query, section.get("keywords", [])),
            "content": self._calculate_content_score(query, section.get("content", "")),
            "bonus": self._calculate_bonus_score(query, section)
        }
    
    def _calculate_total_score(self, scores: Dict[str, float]) -> float:
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        return (scores["title"] * 0.6) + (scores["keyword"] * 0.15) + \
               (scores["content"] * 0.15) + (scores["bonus"] * 0.1)
    
    def _calculate_title_score(self, query: str, title: str) -> float:
        """ì œëª© ë§¤ì¹­ ì ìˆ˜"""
        if not title:
            return 0
            
        query_words = set(query.lower().replace('?', '').replace('!', '').split())
        title_words = set(title.lower().split())
        
        if not query_words:
            return 0
        
        # 1. ì •í™•í•œ ë§¤ì¹­
        exact_matches = len(query_words.intersection(title_words))
        exact_score = exact_matches * 1.0
        
        # 2. ë¶€ë¶„ ë§¤ì¹­
        partial_matches = 0
        for q_word in query_words:
            for t_word in title_words:
                if len(q_word) >= 2 and len(t_word) >= 2:
                    if q_word in t_word or t_word in q_word:
                        partial_matches += 0.6
        
        # 3. í•µì‹¬ í‚¤ì›Œë“œ ë§¤ì¹­
        core_keywords = {
            "ì—”ì§„": ["ì—”ì§„", "engine", "ëª¨í„°"],
            "ì˜¤ì¼": ["ì˜¤ì¼", "oil", "ìœ¤í™œìœ ", "ìœ¤í™œ"],
            "íƒ€ì´ì–´": ["íƒ€ì´ì–´", "tire", "ë°”í€´", "wheel"],
            "ë¸Œë ˆì´í¬": ["ë¸Œë ˆì´í¬", "brake", "ì œë™"],
            "ë°°í„°ë¦¬": ["ë°°í„°ë¦¬", "battery", "ì „ì§€"],
            "í•„í„°": ["í•„í„°", "filter", "ì—¬ê³¼ê¸°"],
            "ë²¨íŠ¸": ["ë²¨íŠ¸", "belt", "íƒ€ì´ë°"],
            "í“¨ì¦ˆ": ["í“¨ì¦ˆ", "fuse", "íœ´ì¦ˆ"],
            "ì „êµ¬": ["ì „êµ¬", "lamp", "light", "ì „ë“±"],
            "ëƒ‰ê°ìˆ˜": ["ëƒ‰ê°ìˆ˜", "ëƒ‰ê°ì•¡", "ì¿¨ëŸ°íŠ¸", "coolant"],
            "êµì²´": ["êµì²´", "êµí™˜", "ê°ˆê¸°", "ë°”ê¾¸ê¸°", "replace", "change"],
            "ì ê²€": ["ì ê²€", "í™•ì¸", "ì²´í¬", "ê²€ì‚¬", "check", "inspect"],
            "ì •ë¹„": ["ì •ë¹„", "ìˆ˜ë¦¬", "ê´€ë¦¬", "maintenance", "service"],
            "ë³´ì¶©": ["ë³´ì¶©", "ì¶”ê°€", "ì¶©ì „", "refill", "add"],
            "ë°©ë²•": ["ë°©ë²•", "ì ˆì°¨", "ê³¼ì •", "ìˆœì„œ", "ë‹¨ê³„", "ë§¤ë‰´ì–¼", "procedure", "how"],
            "ê³ ì¥": ["ê³ ì¥", "ë¬¸ì œ", "ì˜¤ë¥˜", "ì´ìƒ", "trouble", "problem", "fault"],
            "ì‹œë™": ["ì‹œë™", "ì‹œì‘", "start", "ignition"]
        }
        
        direct_matches = 0
        for q_word in query_words:
            for core_key, synonyms in core_keywords.items():
                if q_word in synonyms or q_word == core_key:
                    for t_word in title_words:
                        if t_word in synonyms or t_word == core_key:
                            direct_matches += 1.0
        
        # 4. ì£¼ì œ-ì‘ì—… ì¡°í•© ë§¤ì¹­
        topic_action_combinations = {
            "ì—”ì§„": ["êµì²´", "êµí™˜", "ì ê²€", "í™•ì¸", "ì •ë¹„", "ìˆ˜ë¦¬", "ì˜¤ì¼", "ìœ¤í™œ"],
            "ì˜¤ì¼": ["êµì²´", "êµí™˜", "ì ê²€", "í™•ì¸", "ë³´ì¶©", "ê²Œì´ì§€", "ë ˆë²¨", "ì£¼ì…"],
            "íƒ€ì´ì–´": ["êµì²´", "êµí™˜", "ì ê²€", "í™•ì¸", "ê³µê¸°ì••", "ë§ˆëª¨", "íšŒì „", "ì •ë ¬"],
            "ë¸Œë ˆì´í¬": ["ì ê²€", "í™•ì¸", "êµì²´", "íŒ¨ë“œ", "ë””ìŠ¤í¬", "ì•¡", "ì˜¤ì¼", "ì •ë¹„"],
            "ë°°í„°ë¦¬": ["êµì²´", "ì¶©ì „", "ì ê²€", "í™•ì¸", "ë°©ì „", "ë‹¨ì", "ì²­ì†Œ"],
            "í•„í„°": ["êµì²´", "êµí™˜", "ì²­ì†Œ", "ì ê²€", "ì—ì–´", "ì˜¤ì¼", "ì—°ë£Œ"],
            "ë²¨íŠ¸": ["êµì²´", "êµí™˜", "ì ê²€", "í™•ì¸", "ì¥ë ¥", "íƒ€ì´ë°"],
            "í“¨ì¦ˆ": ["êµì²´", "êµí™˜", "ì ê²€", "í™•ì¸", "ë‹¨ì„ ", "ì°¨ë‹¨"],
            "ì „êµ¬": ["êµì²´", "êµí™˜", "ì ê²€", "í™•ì¸", "ë¨í”„", "ë¼ì´íŠ¸"],
            "ëƒ‰ê°ìˆ˜": ["êµì²´", "ë³´ì¶©", "ì ê²€", "í™•ì¸", "ì˜¨ë„", "ë ˆë²¨"],
            "ì‹œë™": ["ë°©ë²•", "ê±¸ê¸°", "ë„ê¸°", "ë¬¸ì œ", "ê³ ì¥", "í‚¤", "ë²„íŠ¼"]
        }
        
        combination_bonus = 0
        title_lower = title.lower()
        query_lower = query.lower()
        
        for topic, related_actions in topic_action_combinations.items():
            if topic in title_lower:
                matching_actions = sum(1 for action in related_actions if action in query_lower)
                if matching_actions > 0:
                    combination_bonus += min(matching_actions * 0.5, 2.0)
        
        # 5. ìµœì¢… ì ìˆ˜ ê³„ì‚°
        total_matches = exact_score + partial_matches + direct_matches + combination_bonus
        base_score = total_matches / max(len(query_words), 1)
        
        # ì œëª© í’ˆì§ˆ ë³´ë„ˆìŠ¤
        if 2 <= len(title.split()) <= 4 and base_score > 0.5:
            base_score *= 1.2
        
        # ì™„ì „ ì¼ì¹˜ ë³´ë„ˆìŠ¤
        if len(query_words.intersection(title_words)) >= max(len(query_words) - 1, 1):
            base_score *= 1.3
        
        return min(base_score, 2.5)
    
    def _calculate_keyword_score(self, query: str, keywords: List[str]) -> float:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜"""
        if not keywords:
            return 0
        
        query_lower = query.lower()
        query_words = query_lower.split()
        matches = 0
        
        # ì§ì ‘ í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in keywords:
            keyword_lower = keyword.lower()
            if keyword_lower in query_lower:
                matches += 1.0
            elif any(word in keyword_lower or keyword_lower in word for word in query_words if len(word) >= 2):
                matches += 0.5
        
        # ë™ì˜ì–´ ê¸°ë°˜ ë§¤ì¹­
        synonym_dict = self._get_synonyms()
        for keyword in keywords:
            keyword_lower = keyword.lower()
            for query_word in query_words:
                for syn_key, syn_values in synonym_dict.items():
                    if (keyword_lower == syn_key or keyword_lower in syn_values) and \
                       (query_word == syn_key or query_word in syn_values):
                        matches += 0.7
        
        return min(matches / 1.5, 1.0)
    
    def _calculate_content_score(self, query: str, content: str) -> float:
        """ë‚´ìš© ìœ ì‚¬ë„ ì ìˆ˜"""
        try:
            if not content or len(content.strip()) < 20:
                return 0
            
            # ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ ìƒ˜í”Œë§
            if len(content) > 1000:
                content_sample = content[:500] + content[-500:]
            else:
                content_sample = content
            
            query_vector = self.embedding_model.encode_query(query)
            content_vector = self.embedding_model.encode_texts([content_sample])
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            query_norm = query_vector / np.linalg.norm(query_vector, axis=1, keepdims=True)
            content_norm = content_vector / np.linalg.norm(content_vector, axis=1, keepdims=True)
            
            similarity = np.dot(query_norm, content_norm.T)[0][0]
            return max(float(similarity), 0)
            
        except Exception as e:
            print(f"ë‚´ìš© ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0
    
    def _calculate_bonus_score(self, query: str, section: Dict) -> float:
        """íŠ¹ë³„ ë³´ë„ˆìŠ¤ ì ìˆ˜"""
        bonus = 0
        content = section.get("content", "").lower()
        title = section.get("title", "").lower()
        query_lower = query.lower()
        
        # ì£¼ì œë³„ ê´€ë ¨ì„± ë³´ë„ˆìŠ¤
        topic_bonuses = {
            ("ì—”ì§„", "ì˜¤ì¼"): ["êµì²´", "êµí™˜", "ë°©ë²•", "ì ˆì°¨", "ì ê²€"],
            ("íƒ€ì´ì–´",): ["êµì²´", "êµí™˜", "ì ê²€", "ê³µê¸°ì••", "ë§ˆëª¨"],
            ("ë¸Œë ˆì´í¬",): ["ì ê²€", "í™•ì¸", "êµì²´", "íŒ¨ë“œ", "ì•¡"],
            ("ë°°í„°ë¦¬",): ["êµì²´", "ì¶©ì „", "ì ê²€", "í™•ì¸", "ë°©ì „"],
            ("ì‹œë™",): ["ë°©ë²•", "ê±¸ê¸°", "ë„ê¸°", "ë¬¸ì œ", "ê³ ì¥"],
            ("ì •ë¹„", "ìˆ˜ë¦¬"): ["ë°©ë²•", "ì ˆì°¨", "ì ê²€", "êµì²´"],
            ("í•„í„°",): ["êµì²´", "êµí™˜", "ì²­ì†Œ", "ì ê²€"],
            ("ëƒ‰ê°ìˆ˜", "ì¿¨ëŸ°íŠ¸"): ["êµì²´", "ë³´ì¶©", "ì ê²€", "í™•ì¸"],
            ("ì˜¤ì¼",): ["êµì²´", "êµí™˜", "ë³´ì¶©", "ì ê²€", "ê²Œì´ì§€"]
        }
        
        for title_keywords, action_keywords in topic_bonuses.items():
            if any(kw in title for kw in title_keywords):
                matching_actions = sum(1 for action in action_keywords if action in query_lower)
                if matching_actions > 0:
                    bonus += min(matching_actions * 0.15, 0.3)
        
        # ì ˆì°¨ì  ë‚´ìš© ë³´ë„ˆìŠ¤
        if any(word in query_lower for word in ["ë°©ë²•", "ì ˆì°¨", "ë‹¨ê³„", "ê³¼ì •"]):
            import re
            steps = len(re.findall(r'\d+\.\s', content))
            if steps >= 3:
                bonus += 0.2
            elif steps >= 1:
                bonus += 0.1
        
        # ê´€ë ¨ í‚¤ì›Œë“œ ë°€ë„ ë³´ë„ˆìŠ¤
        important_words = [
            "êµì²´", "êµí™˜", "ì ê²€", "í™•ì¸", "ë³´ì¶©", "ê²Œì´ì§€", "ì£¼ì…êµ¬",
            "ë°©ë²•", "ì ˆì°¨", "ë‹¨ê³„", "ê³¼ì •", "ì£¼ì˜", "ê²½ê³ ", "ì•ˆì „"
        ]
        word_count = sum(1 for word in important_words if word in content)
        bonus += min(word_count * 0.03, 0.15)
        
        # ì œëª© í’ˆì§ˆ ë³´ë„ˆìŠ¤
        if 2 <= len(title.split()) <= 5:
            bonus += 0.1
        
        # ë‚´ìš© ê¸¸ì´ ì ì •ì„± ë³´ë„ˆìŠ¤
        content_length = len(section.get("content", ""))
        if 200 <= content_length <= 2000:
            bonus += 0.1
        elif content_length > 2000:
            bonus -= 0.05
        
        return min(bonus, 1.0)
    
    def _get_synonyms(self) -> Dict[str, List[str]]:
        """ë™ì˜ì–´ ì‚¬ì „"""
        return {
            "ì—”ì§„": ["ì—”ì§„", "ëª¨í„°", "engine"],
            "ì˜¤ì¼": ["ì˜¤ì¼", "ìœ¤í™œìœ ", "ìœ¤í™œ", "oil", "lubricant"],
            "ì—”ì§„ì˜¤ì¼": ["ì—”ì§„ì˜¤ì¼", "ì—”ì§„ ì˜¤ì¼", "ëª¨í„°ì˜¤ì¼", "ìœ¤í™œìœ "],
            "êµì²´": ["êµí™˜", "ê°ˆê¸°", "ë³€ê²½", "ë°”ê¾¸ê¸°", "ìˆ˜ë¦¬", "replace", "change"],
            "ì ê²€": ["í™•ì¸", "ì²´í¬", "ê²€ì‚¬", "ì¸¡ì •", "check", "inspect"],
            "ì •ë¹„": ["ìˆ˜ë¦¬", "ê´€ë¦¬", "maintenance", "service"],
            "ë³´ì¶©": ["ì¶”ê°€", "ì¶©ì „", "refill", "add"],
            "ë°©ë²•": ["ì ˆì°¨", "ê³¼ì •", "ìˆœì„œ", "ë‹¨ê³„", "ë§¤ë‰´ì–¼", "ì•ˆë‚´", "procedure"],
            "ë‹¨ê³„": ["ì ˆì°¨", "ê³¼ì •", "ìˆœì„œ", "step"],
            "íƒ€ì´ì–´": ["íƒ€ì´ì–´", "ë°”í€´", "tire", "wheel"],
            "ë¸Œë ˆì´í¬": ["ë¸Œë ˆì´í¬", "ì œë™", "brake"],
            "ë°°í„°ë¦¬": ["ë°°í„°ë¦¬", "ì „ì§€", "battery"],
            "í•„í„°": ["í•„í„°", "ì—¬ê³¼ê¸°", "filter"],
            "ë²¨íŠ¸": ["ë²¨íŠ¸", "íƒ€ì´ë°ë²¨íŠ¸", "belt"],
            "í˜¸ìŠ¤": ["í˜¸ìŠ¤", "íŒŒì´í”„", "hose", "pipe"],
            "ëƒ‰ê°ìˆ˜": ["ëƒ‰ê°ì•¡", "ì¿¨ëŸ°íŠ¸", "coolant", "antifreeze"],
            "ë¸Œë ˆì´í¬ì•¡": ["ë¸Œë ˆì´í¬ì˜¤ì¼", "ë¸Œë ˆì´í¬ì•¡", "brake fluid"],
            "ì›Œì…”ì•¡": ["ì„¸ì •ì•¡", "washer fluid"],
            "ê³ ì¥": ["ë¬¸ì œ", "ì˜¤ë¥˜", "ì´ìƒ", "trouble", "problem"],
            "ê²½ê³ ": ["ì£¼ì˜", "warning", "caution"],
            "ì•ˆì „": ["safety", "ë³´ì•ˆ"],
            "ì‹œë™": ["ì‹œì‘", "start", "ignition"],
            "ì •ì§€": ["ì¤‘ì§€", "stop", "turn off"],
            "ê³„ê¸°íŒ": ["ëŒ€ì‹œë³´ë“œ", "dashboard", "cluster"],
            "ê²Œì´ì§€": ["ì¸¡ì •ê¸°", "gauge", "meter"],
            "í‘œì‹œë“±": ["ë¨í”„", "light", "indicator"],
            "ì—”ì§„ë£¸": ["ë³´ë‹›", "hood", "engine bay"],
            "íŠ¸ë í¬": ["ì ì¬í•¨", "trunk", "cargo"],
            "ì‹¤ë‚´": ["cabin", "interior"],
            "ì—ì–´ì»¨": ["ëƒ‰ë°©", "air conditioning", "A/C"],
            "íˆí„°": ["ë‚œë°©", "heating"],
            "ì™€ì´í¼": ["ì™€ì´ì…”", "wiper"],
            "ì—°ë£Œ": ["ê¸°ë¦„", "ê°€ì†”ë¦°", "fuel", "gasoline"],
            "ì£¼ìœ ": ["ê¸‰ìœ ", "fueling", "refueling"]
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        return {
            "documents_count": len(self.documents),
            "total_sections": sum(len(doc.get("sections", [])) for doc in self.documents)
        }